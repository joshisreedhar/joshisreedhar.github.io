---
layout: post
title: Embeddings, what they are
---

"Embeddings" is a keyword that we keep hearing in the space of Generative AI. This article will try to provide a brief explanation of what embeddings are. When we train a model, we use a dataset. The inputs contained in the dataset can be categorized into Continuous and Categorical inputs. An example of a continuous input dataset could be a Netflix show and corresponding rating. The rating indicates the value of a particular observation, and there is an associated ranking. Let's say a crime thriller show has a rating of 2, and a comedy show has a rating of 4. In a loose sense, the model will derive a conclusion that I, as a user, prefer comedy over crime thriller because inherently 4 > 2.

Replace this example with, let's say, I have 5 hats, one of each color: red, blue, green, white, and black. I wear the black one the most. Here the concept of color has no built-in implicit value of one over the other. They are just features of inputs. We could encode this information into a matrix with one column representing each color and set a value of 1 for the corresponding column and 0 for the remaining. In the above example, I will have a matrix of 6 X Observation Count, and for each observation, we would have something like [work, 0, 0, 0, 0, 1], [park, 0, 0, 0, 0, 1], where 0 and 1 indicate which color hat I wore. By the way, this way of converting categorical data into a matrix multiplication is called One-hot encoding. The challenge with this, though, is that for high cardinality (possible unique values) data, the matrix will become large and sparse (mostly 0s), and it becomes ineffective for the model to learn.

In the ML world, the way they solve this issue is by resorting to matrix multiplication. Let's say you have a matrix of 1000X500 (i.e., 1000 observations for 500 encoded features). Let's say we want to collapse this to 50 features. All I have to do is devise a 500 X 50 matrix and multiply it to get a 1000 X 50 matrix. The 500 X 50 matrix is basically translating the 0s and 1s of 500 features into some unique decimal values of the 50 features. This smaller matrix we described is called feature embeddings. Based on the above example, we can think of translating literals/tokens to a number representation in a vector form, and hence the tokens/text/literals will have similar embeddings. Each LLM has methods to create embeddings for a given text. Some of the use cases of embeddings are to determine similarity; if two statements are semantically related, the vector dot product of the corresponding vector embeddings will be closer to 0.

So, in summary, embeddings are a mechanism for converting a piece of text to its decimal vector representation so that you can determine the semantic relatedness of two pieces of information.
